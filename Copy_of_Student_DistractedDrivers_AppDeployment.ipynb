{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arnav-1Y/Machine-Learning/blob/main/Copy_of_Student_DistractedDrivers_AppDeployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ1vhmV3gRAb"
      },
      "source": [
        "#Deploying an App with Streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D9qJyHzfVTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YglDk0H7Sbyj"
      },
      "source": [
        "![](https://raw.githubusercontent.com/NolanChai/model_repo/main/Distracted_Driving.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idhcUBBrT3xy"
      },
      "source": [
        "Congratulations for making it this far in the project! So far, we have:\n",
        "- Trained a computer vision model to detect whether or not a driver is attentive to the road\n",
        "- Generated saliency maps for our predictions\n",
        "- Fine tuned expert models via transfer learning\n",
        "\n",
        "In this notebook, we will be:\n",
        "- Deploying our AI model on Streamlit\n",
        "- Customizing the user interface!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKQQxfpQhX9-"
      },
      "source": [
        "# Part 1. Streamlit - Deploying your model to the web\n",
        "Today we will be using Streamlit, a framework to easily build web applications, to deploy our models to the web so that they can be shared to the web!\n",
        "\n",
        "Take a moment to look through examples of websites built with Streamlit [here](https://streamlit.io/gallery?category=favorites). As a class, choose your favorite and answer the following **questions:**\n",
        "* Who is this application for?\n",
        "* How does the user input data - are these intuitive ways of interacting with the app?\n",
        "* What does the application do with the data?\n",
        "* Evaluate the ease of use and look of the application.\n",
        "\n",
        "Now that we’ve seen what is possible with Streamlit, let’s try to deploy our **Distracted Drivers model** to the web!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s1sO0FM7pbY"
      },
      "source": [
        "First, let's make sure that we are using our GPU! <br> This block of code should output `Found GPU at: /device:GPU:0`. <br> If not, go to your `Runtime` tab, and `Change Runtime`.\n",
        "- Select a T4 GPU or TPU under the Hardware Accelerator section!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvUx1cAG7rX0",
        "outputId": "337859fc-5432-4b9f-f15e-3caf6df365ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('No GPU Found! :(')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo pip install tensorflow keras\n",
        "# or install with the optional dependency group\n",
        "!sudo pip install scikeras[tensorflow]\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "#!sudo pip install\n",
        "#!sudo pip install keras\n",
        "#!sudo pip install tensorflow[and-cuda]\n",
        "#print(tf.__version__)\n",
        "#print(keras.__version__)"
      ],
      "metadata": {
        "id": "k49SV8wdGWNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc6402e-c0f2-4bf4-d688-b26b2dce9073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.78.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.18.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: scikeras[tensorflow] in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras[tensorflow]) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras[tensorflow]) (1.6.1)\n",
            "Requirement already satisfied: tensorflow>=2.16.1 in /usr/local/lib/python3.12/dist-packages (from scikeras[tensorflow]) (2.19.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras[tensorflow]) (2.1.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras[tensorflow]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras[tensorflow]) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras[tensorflow]) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras[tensorflow]) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras[tensorflow]) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras[tensorflow]) (26.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras[tensorflow]) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras[tensorflow]) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras[tensorflow]) (3.6.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (1.78.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16.1->scikeras[tensorflow]) (2.19.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.16.1->scikeras[tensorflow]) (0.46.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16.1->scikeras[tensorflow]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16.1->scikeras[tensorflow]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16.1->scikeras[tensorflow]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16.1->scikeras[tensorflow]) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16.1->scikeras[tensorflow]) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16.1->scikeras[tensorflow]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16.1->scikeras[tensorflow]) (3.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras[tensorflow]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras[tensorflow]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras[tensorflow]) (0.1.2)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.16.1->scikeras[tensorflow]) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9fUzzYUSSmM",
        "outputId": "29d3b226-5e51-4471-f990-59e60214189e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "`np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3080122939.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugmenters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mNP_INT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imgaug/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# classes/functions, hence always place the other imports below this so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# the deprecated stuff gets overwritten as much as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgaug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maugmentables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imgaug/imgaug.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# `dtype.type in  NP_FLOAT_TYPES` do not just use `dtype in NP_FLOAT_TYPES` as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# that would fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mNP_FLOAT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mNP_INT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mNP_UINT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"uint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__expired_attributes__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0;34mf\"`np.{attr}` was removed in the NumPy 2.0 release. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"{__expired_attributes__[attr]}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: `np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead."
          ]
        }
      ],
      "source": [
        "#@title Run this to set up! (This may take 2-3 minutes)\n",
        "\n",
        "# Hide Warnings -------------------------------------------\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "print(sys.version)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class HiddenPrints:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "# Installations --------------------------------------------\n",
        "with HiddenPrints():\n",
        "    #!pip update -y\n",
        "    #!pip install python3.12 python3.12-distutils -y\n",
        "    !sudo pip -q install streamlit\n",
        "    !sudo pip -q install pyngrok\n",
        "    # Uninstall existing numpy that might conflict, then install compatible version for imgaug\n",
        "    !sudo pip uninstall numpy -y\n",
        "    !sudo pip install numpy==1.26.6\n",
        "    !sudo pip install imgaug\n",
        "    # Install core ML libraries, which should now respect the installed numpy\n",
        "    !sudo pip install tensorflow keras\n",
        "# or install with the optional dependency group\n",
        "    !sudo pip install scikeras[tensorflow]\n",
        "    # Add installation for tf-keras-vis, which is used for saliency map\n",
        "    !sudo pip install tf-keras-vis\n",
        "    !wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Driver%20Distraction%20Detection/metadata.csv'\n",
        "    !wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Driver%20Distraction%20Detection/image_data.npy'\n",
        "    # Want to move these to a bucket\n",
        "    !wget https://huggingface.co/NolanChai/Inspirit_Expert_Models/resolve/main/DenseNet121.h5\n",
        "    !wget https://huggingface.co/NolanChai/Inspirit_Expert_Models/resolve/main/ResNet50.h5\n",
        "    !wget https://huggingface.co/NolanChai/Inspirit_Expert_Models/resolve/main/vgg16.h5\n",
        "    !wget https://huggingface.co/NolanChai/Inspirit_Expert_Models/resolve/main/vgg19.h5\n",
        "\n",
        "# Imports --------------------------------------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import streamlit\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyngrok import ngrok\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import model_selection\n",
        "from collections import Counter\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D\n",
        "#!pip install -q git+https://github.com/rdk2132/scikeras # workaround for scikeras deprecation\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import VGG16, VGG19, ResNet50, DenseNet121\n",
        "from imgaug import augmenters\n",
        "\n",
        "NP_INT_TYPES = {np.int8, np.int16, np.int32, np.int64}\n",
        "NP_UINT_TYPES = {np.uint8, np.uint16, np.uint32, np.uint64}\n",
        "NP_FLOAT_TYPES = {np.float16, np.float32, np.float64}\n",
        "\n",
        "# Helper Functions ------------------------------------------\n",
        "def label_to_numpy(labels):\n",
        "  final_labels = np.zeros((len(labels), 4))\n",
        "  for i in range(len(labels)):\n",
        "    label = labels[i]\n",
        "    if label == 'Attentive':\n",
        "      final_labels[i,:] = np.array([1, 0, 0, 0])\n",
        "    if label == 'DrinkingCoffee':\n",
        "      final_labels[i,:] = np.array([0, 1, 0, 0])\n",
        "    if label == 'UsingMirror':\n",
        "      final_labels[i,:] = np.array([0, 0, 1, 0])\n",
        "    if label == 'UsingRadio':\n",
        "      final_labels[i,:] = np.array([0, 0, 0, 1])\n",
        "  return final_labels\n",
        "\n",
        "def augment(data, augmenter):\n",
        "  if len(data.shape) == 3:\n",
        "    return augmenter.augment_image(data)\n",
        "  if len(data.shape) == 4:\n",
        "    return augmenter.augment_images(data)\n",
        "\n",
        "def rotate(data, rotate):\n",
        "  fun = augmenters.Affine(rotate = rotate)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def shear(data, shear):\n",
        "  fun = augmenters.Affine(shear = shear)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def scale(data, scale):\n",
        "  fun = augmenters.Affine(scale = shear)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def flip_left_right(data):\n",
        "  fun = augmenters.Fliplr()\n",
        "  return augment(data, fun)\n",
        "\n",
        "def flip_up_down(data):\n",
        "  fun = augmenters.Flipud()\n",
        "  return augment(data, fun)\n",
        "\n",
        "def remove_color(data, channel):\n",
        "  new_data = data.copy()\n",
        "  if len(data.shape) == 3:\n",
        "    new_data[:,:,channel] = 0\n",
        "    return new_data\n",
        "  if len(data.shape) == 4:\n",
        "    new_data[:,:,:,channel] = 0\n",
        "    return new_data\n",
        "\n",
        "class pkg:\n",
        "  #### DOWNLOADING AND LOADING DATA\n",
        "  def get_metadata(metadata_path, which_splits = ['train', 'test']):\n",
        "    '''returns metadata dataframe which contains columns of:\n",
        "       * index: index of data into numpy data\n",
        "       * class: class of image\n",
        "       * split: which dataset split is this a part of?\n",
        "    '''\n",
        "    metadata = pd.read_csv(metadata_path)\n",
        "    keep_idx = metadata['split'].isin(which_splits)\n",
        "    metadata = metadata[keep_idx]\n",
        "\n",
        "    # Get dataframes for each class.\n",
        "    df_coffee_train = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_coffee_test = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_mirror_train = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_mirror_test = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_attentive_train = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_attentive_test = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_radio_train = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_radio_test = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "\n",
        "    # Get number of items in class with lowest number of images.\n",
        "    num_samples_train = min(df_coffee_train.shape[0], \\\n",
        "                            df_mirror_train.shape[0], \\\n",
        "                            df_attentive_train.shape[0], \\\n",
        "                            df_radio_train.shape[0])\n",
        "    num_samples_test = min(df_coffee_test.shape[0], \\\n",
        "                            df_mirror_test.shape[0], \\\n",
        "                            df_attentive_test.shape[0], \\\n",
        "                            df_radio_test.shape[0])\n",
        "\n",
        "    # Resample each of the classes and concatenate the images.\n",
        "    metadata_train = pd.concat([df_coffee_train.sample(num_samples_train), \\\n",
        "                          df_mirror_train.sample(num_samples_train), \\\n",
        "                          df_attentive_train.sample(num_samples_train), \\\n",
        "                          df_radio_train.sample(num_samples_train) ])\n",
        "    metadata_test = pd.concat([df_coffee_test.sample(num_samples_test), \\\n",
        "                          df_mirror_test.sample(num_samples_test), \\\n",
        "                          df_attentive_test.sample(num_samples_test), \\\n",
        "                          df_radio_test.sample(num_samples_test) ])\n",
        "\n",
        "    metadata = pd.concat( [metadata_train, metadata_test] )\n",
        "\n",
        "    return metadata\n",
        "\n",
        "  def get_data_split(split_name, flatten, all_data, metadata, image_shape):\n",
        "    '''\n",
        "    returns images (data), labels from folder of format [image_folder]/[split_name]/[class_name]/\n",
        "    flattens if flatten option is True\n",
        "    '''\n",
        "    # Get dataframes for each class.\n",
        "    df_coffee_train = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_coffee_test = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_mirror_train = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_mirror_test = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_attentive_train = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_attentive_test = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_radio_train = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_radio_test = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "\n",
        "    # Get number of items in class with lowest number of images.\n",
        "    num_samples_train = min(df_coffee_train.shape[0], \\\n",
        "                            df_mirror_train.shape[0], \\\n",
        "                            df_attentive_train.shape[0], \\\n",
        "                            df_radio_train.shape[0])\n",
        "    num_samples_test = min(df_coffee_test.shape[0], \\\n",
        "                            df_mirror_test.shape[0], \\\n",
        "                            df_attentive_test.shape[0], \\\n",
        "                            df_radio_test.shape[0])\n",
        "\n",
        "    # Resample each of the classes and concatenate the images.\n",
        "    metadata_train = pd.concat([df_coffee_train.sample(num_samples_train), \\\n",
        "                          df_mirror_train.sample(num_samples_train), \\\n",
        "                          df_attentive_train.sample(num_samples_train), \\\n",
        "                          df_radio_train.sample(num_samples_train) ])\n",
        "    metadata_test = pd.concat([df_coffee_test.sample(num_samples_test), \\\n",
        "                          df_mirror_test.sample(num_samples_test), \\\n",
        "                          df_attentive_test.sample(num_samples_test), \\\n",
        "                          df_radio_test.sample(num_samples_test) ])\n",
        "\n",
        "    metadata = pd.concat( [metadata_train, metadata_test] )\n",
        "\n",
        "    sub_df = metadata[metadata['split'].isin([split_name])]\n",
        "    index  = sub_df['index'].values\n",
        "    labels = sub_df['class'].values\n",
        "    data = all_data[index,:]\n",
        "    if flatten:\n",
        "      data = data.reshape([-1, np.product(image_shape)])\n",
        "    return data, labels\n",
        "\n",
        "  def get_train_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('train', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_test_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('test', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_field_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('field', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "class helpers:\n",
        "  #### PLOTTING\n",
        "  def plot_image(data, num_ims, figsize=(8,6), labels = [], index = None, image_shape = [64,64,3]):\n",
        "    '''\n",
        "    if data is a single image, display that image\n",
        "\n",
        "    if data is a 4d stack of images, display that image\n",
        "    '''\n",
        "    print(data.shape)\n",
        "    num_dims   = len(data.shape)\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    # reshape data if necessary\n",
        "    if num_dims == 1:\n",
        "      data = data.reshape(target_shape)\n",
        "    if num_dims == 2:\n",
        "      data = data.reshape(-1,image_shape[0],image_shape[1],image_shape[2])\n",
        "    num_dims   = len(data.shape)\n",
        "\n",
        "    # check if single or multiple images\n",
        "    if num_dims == 3:\n",
        "      if num_labels > 1:\n",
        "        print('Multiple labels does not make sense for single image.')\n",
        "        return\n",
        "\n",
        "      label = labels\n",
        "      if num_labels == 0:\n",
        "        label = ''\n",
        "      image = data\n",
        "\n",
        "    if num_dims == 4:\n",
        "      image = data[index, :]\n",
        "      label = labels[index]\n",
        "\n",
        "    # plot image of interest\n",
        "\n",
        "    nrows=int(np.sqrt(num_ims))\n",
        "    ncols=int(np.ceil(num_ims/nrows))\n",
        "    print(nrows,ncols)\n",
        "    count=0\n",
        "    if nrows==1 and ncols==1:\n",
        "      print('Label: %s'%label)\n",
        "      plt.imshow(image)\n",
        "      plt.show()\n",
        "    else:\n",
        "      print(labels)\n",
        "      fig = plt.figure(figsize=figsize)\n",
        "      for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "          if count<num_ims:\n",
        "            fig.add_subplot(nrows,ncols,count+1)\n",
        "            plt.imshow(image[count])\n",
        "            count+=1\n",
        "      fig.set_size_inches(18.5, 10.5)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  #### QUERYING AND COMBINING DATA\n",
        "  def get_misclassified_data(data, labels, predictions):\n",
        "    '''\n",
        "    Gets the data and labels that are misclassified in a classification task\n",
        "    Returns:\n",
        "    -missed_data\n",
        "    -missed_labels\n",
        "    -predicted_labels (corresponding to missed_labels)\n",
        "    -missed_index (indices of items in original dataset)\n",
        "    '''\n",
        "    missed_index     = np.where(np.abs(predictions.squeeze() - labels.squeeze()) > 0)[0]\n",
        "    missed_labels    = labels[missed_index]\n",
        "    missed_data      = data[missed_index,:]\n",
        "    predicted_labels = predictions[missed_index]\n",
        "    return missed_data, missed_labels, predicted_labels, missed_index\n",
        "\n",
        "  def combine_data(data_list, labels_list):\n",
        "    return np.concatenate(data_list, axis = 0), np.concatenate(labels_list, axis = 0)\n",
        "\n",
        "  def model_to_string(model):\n",
        "    import re\n",
        "    stringlist = []\n",
        "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "\n",
        "    for layer in model.layers:\n",
        "      if hasattr(layer,\"activation\"):\n",
        "        stringlist.append(str(layer.activation))\n",
        "\n",
        "    sms = \"\\n\".join(stringlist)\n",
        "    sms = re.sub(r'_\\d\\d\\d','', sms)\n",
        "    sms = re.sub(r'_\\d\\d','', sms)\n",
        "    sms = re.sub(r'_\\d','', sms)\n",
        "    return sms\n",
        "\n",
        "  def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n",
        "    # i'm sorry for this function's code. i am so sorry.\n",
        "    history = history.history\n",
        "    history.update({'epoch':list(range(len(history['val_accuracy'])))})\n",
        "    history = pd.DataFrame.from_dict(history)\n",
        "\n",
        "    best_epoch = history.sort_values(by = 'val_accuracy', ascending = False).iloc[0]['epoch']\n",
        "\n",
        "    if not ax:\n",
        "      f, ax = plt.subplots(1,1)\n",
        "    sns.lineplot(x = 'epoch', y = 'val_accuracy', data = history, label = 'Validation', ax = ax)\n",
        "    sns.lineplot(x = 'epoch', y = 'accuracy', data = history, label = 'Training', ax = ax)\n",
        "    ax.axhline(0.25, linestyle = '--',color='red', label = 'Chance')\n",
        "    ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')\n",
        "    ax.legend(loc = 1)\n",
        "    ax.set_ylim([0.01, 1])\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Accuracy (Fraction)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class models:\n",
        "  def DenseClassifier(hidden_layer_sizes, nn_params, dropout = 0.5):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape = nn_params['input_shape']))\n",
        "    for ilayer in hidden_layer_sizes:\n",
        "      model.add(Dense(ilayer, activation = 'relu'))\n",
        "      if dropout:\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(units = nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "    model.compile(loss=nn_params['loss'],\n",
        "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.95),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def CNNClassifier(num_hidden_layers, nn_params, dropout = 0.5):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=nn_params['input_shape'], padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    for i in range(num_hidden_layers-1):\n",
        "        model.add(Conv2D(32, (3, 3), padding = 'same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(units = 128, activation = 'relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(units = 64, activation = 'relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(units = nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "    # initiate RMSprop optimizer\n",
        "    opt = tensorflow.keras.optimizers.RMSprop(lr=1e-4)\n",
        "\n",
        "    # Let's train the model using RMSprop\n",
        "    model.compile(loss=nn_params['loss'],\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def TransferClassifier(name, nn_params, trainable = True):\n",
        "    expert_dict = {'VGG16': VGG16,\n",
        "                   'VGG19': VGG19,\n",
        "                   'ResNet50':ResNet50,\n",
        "                   'DenseNet121':DenseNet121}\n",
        "\n",
        "    expert_conv = expert_dict[name](weights = 'imagenet',\n",
        "                                              include_top = False,\n",
        "                                              input_shape = nn_params['input_shape'])\n",
        "    for layer in expert_conv.layers:\n",
        "      layer.trainable = trainable\n",
        "\n",
        "    expert_model = Sequential()\n",
        "    expert_model.add(expert_conv)\n",
        "    expert_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    expert_model.add(Dense(128, activation = 'relu'))\n",
        "    expert_model.add(Dropout(0.3))\n",
        "\n",
        "    expert_model.add(Dense(64, activation = 'relu'))\n",
        "\n",
        "    expert_model.add(Dense(nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "    expert_model.compile(loss = nn_params['loss'],\n",
        "                  optimizer = optimizers.SGD(lr=1e-4, momentum=0.95),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return expert_model\n",
        "\n",
        "### defining project variables\n",
        "# file variables\n",
        "# image_data_url       = 'https://drive.google.com/uc?id=1qmTuUyn0525-612yS-wkp8gHB72Wv_XP'\n",
        "# metadata_url         = 'https://drive.google.com/uc?id=1OfKnq3uIT29sXjWSZqOOpceig8Ul24OW'\n",
        "image_data_path      = './image_data.npy'\n",
        "metadata_path        = './metadata.csv'\n",
        "image_shape          = (64, 64, 3)\n",
        "\n",
        "# neural net parameters\n",
        "nn_params = {}\n",
        "nn_params['input_shape']       = image_shape\n",
        "nn_params['output_neurons']    = 4\n",
        "nn_params['loss']              = 'categorical_crossentropy'\n",
        "nn_params['output_activation'] = 'softmax'\n",
        "\n",
        "### pre-loading all data of interest\n",
        "_all_data = np.load('image_data.npy')\n",
        "_metadata = pkg.get_metadata(metadata_path, ['train','test','field'])\n",
        "\n",
        "### preparing definitions\n",
        "# downloading and loading data\n",
        "get_data_split = pkg.get_data_split\n",
        "get_metadata    = lambda :                 pkg.get_metadata(metadata_path, ['train','test'])\n",
        "get_train_data  = lambda flatten = False : pkg.get_train_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_test_data   = lambda flatten = False : pkg.get_test_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_field_data  = lambda flatten = False : pkg.get_field_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "\n",
        "# plotting\n",
        "plot_image = lambda data, num_ims,figsize=(8,6), labels = [], index = None: helpers.plot_image(data = data, num_ims=num_ims, figsize=figsize,labels = labels, index = index, image_shape = image_shape);\n",
        "plot_acc       = lambda history: helpers.plot_acc(history)\n",
        "\n",
        "# querying and combining data\n",
        "model_to_string        = lambda model: helpers.model_to_string(model)\n",
        "get_misclassified_data = helpers.get_misclassified_data;\n",
        "combine_data           = helpers.combine_data;\n",
        "\n",
        "# models with input parameters\n",
        "DenseClassifier     = lambda hidden_layer_sizes: models.DenseClassifier(hidden_layer_sizes = hidden_layer_sizes, nn_params = nn_params);\n",
        "CNNClassifier       = lambda num_hidden_layers: models.CNNClassifier(num_hidden_layers, nn_params = nn_params);\n",
        "TransferClassifier  = lambda name: models.TransferClassifier(name = name, nn_params = nn_params);\n",
        "\n",
        "#monitor = ModelCheckpoint('./model.h5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "monitor = ModelCheckpoint('./model.h5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "\n",
        "print(\"Setup Successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b014508d"
      },
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AI prompt cell\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Markdown,clear_output\n",
        "from google.colab import ai\n",
        "\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=[],\n",
        "    layout={'width': 'auto'}\n",
        ")\n",
        "\n",
        "def update_model_list(new_options):\n",
        "    dropdown.options = new_options\n",
        "update_model_list(ai.list_models())\n",
        "\n",
        "text_input = widgets.Textarea(\n",
        "    placeholder='Ask me anything....',\n",
        "    layout={'width': 'auto', 'height': '100px'},\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='Submit Text',\n",
        "    disabled=False,\n",
        "    tooltip='Click to submit the text',\n",
        "    icon='check'\n",
        ")\n",
        "\n",
        "output_area = widgets.Output(\n",
        "     layout={'width': 'auto', 'max_height': '300px','overflow_y': 'scroll'}\n",
        ")\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output(wait=False)\n",
        "        accumulated_content = \"\"\n",
        "        for new_chunk in ai.generate_text(prompt=text_input.value, model_name=dropdown.value, stream=True):\n",
        "            if new_chunk is None:\n",
        "                continue\n",
        "            accumulated_content += new_chunk\n",
        "            clear_output(wait=True)\n",
        "            display(Markdown(accumulated_content))\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "vbox = widgets.GridBox([dropdown, text_input, button, output_area])\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<style>\n",
        ".widget-dropdown select {\n",
        "    font-size: 18px;\n",
        "    font-family: \"Arial\", sans-serif;\n",
        "}\n",
        ".widget-textarea textarea {\n",
        "    font-size: 18px;\n",
        "    font-family: \"Arial\", sans-serif;\n",
        "}\n",
        "</style>\n",
        "\"\"\"))\n",
        "display(vbox)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g61V9wEi55NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh1xQ9kMWPCi"
      },
      "source": [
        "In the meantime, **discuss**: How might you want to design a website using our machine learning model? What kind of visuals or graphs would be useful to your user?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n55hQj2rJ7v2"
      },
      "source": [
        "## Load Your Models into Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX8_NQ0CXlr_"
      },
      "source": [
        "Let's load the models we've trained so far into Streamlit! We can do this by using the tensorflow library. If you open up the files tab (the folder icon on the left sidebar), you can see that we have a `.h5` tensor files downloaded that stores our trained parameters for each of our expert models from Notebook 2 (ResNet50, VGG16, VGG19, DenseNet121). Feel free to also upload your own saved model from any of the previous notebooks (`model.h5`) to try using!\n",
        "\n",
        "Here is a reference on how to save and load sklearn and tensorflow models!\n",
        "\n",
        "For `sklearn`:\n",
        "```python\n",
        "from joblib import dump, load\n",
        "\n",
        "# ====== Save model ========\n",
        "dump(model, 'filename.joblib')\n",
        "\n",
        "# ====== Load model ========\n",
        "clf = load('filename.joblib')\n",
        "```\n",
        "\n",
        "For `tensorflow`:\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tf.keras.models import load_model\n",
        "\n",
        "# ====== Save model ========\n",
        "model.save(\"filename.h5\")\n",
        "\n",
        "# ====== Load model ========\n",
        "tensorflow.keras.models.load_model(\"filename.h5\")\n",
        "```\n",
        "\n",
        "Our model today is going to be an sklearn model! Let's load in the model we created last time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s98DjuZ1XeHe"
      },
      "outputs": [],
      "source": [
        "file_path = 'vgg16.h5'\n",
        "model = load_model(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a644whDkX8Iq"
      },
      "source": [
        "When dealing with Streamlit and web development, we begin to work with **file management**. In Python web development, such as Flask and Streamlit, we use a central Python file to launch our app. In this case, we will use `app.py`.\n",
        "```\n",
        "app.py\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtZHuP-cdJ0S"
      },
      "source": [
        "Using Streamlit, we can build a simple webapp like so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdOdJ-ODdahf"
      },
      "source": [
        "```\n",
        "%%writefile app.py\n",
        "```\n",
        "The `%%writefile` command writes to a file (and creates one if it doesn't already exist!). Everything that follows the rest of this block of code will be written to `app.py`. <br> In this case, we use this command to create our `app.py` file, which you can check in the left sidebar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTPwvXvcdPh3"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "st.title('Hello World')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHaXdm2yne2G"
      },
      "source": [
        "To initiate our `app.py`, we employ both Streamlit and ngrok in tandem. Ngrok serves as a hosting service that enables us to establish secure tunnels to our localhost, making our local server accessible over the internet. Streamlit, on the other hand, is a powerful library designed to turn Python scripts into interactive web applications easily. By integrating Streamlit with ngrok, we create a bridge that connects our Python application to the web, allowing external access.\n",
        "\n",
        "<br> To get access to our Ngrok server, we need to sign up on their website and get a unique **authentication token**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxdAMUKRn5mj"
      },
      "source": [
        "<font color=SlateGrey><h2><b>\n",
        "Use [these](https://drive.google.com/file/d/12zwuOuKh91VSHIHS-6S4ADF4HLC2wKJq/view?usp=sharing) instructions to create a ngrok account and get your authtoken!\n",
        "</b></h2></font>\n",
        "\n",
        "<font color=DarkGray><h3><b>\n",
        "Paste your authtoken below next to `!ngrok authtoken`!\n",
        "</b></h3></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fbg8jwYn5mp"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken 2iTDRG67TyKZEUZMQCVYiV9ZcLY_3AumQRAPhn9vgJuSRYL8x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTuX65aQpS3Z"
      },
      "source": [
        "Now, we can launch our website through the `launch_website()` function we have written below. We can connect our ngrok token by building a 'tunnel' to our Streamlit code. However, with the free plan, we cannot have more than one tunnel to the server.\n",
        "```\n",
        "if ngrok.get_tunnels():\n",
        "    ngrok.kill()\n",
        "```\n",
        "ensures that more than one instance of your website is not running at once! (i.e., If there is an existing 'tunnel' already running, it will 'kill' it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL7GDq2eiV0h"
      },
      "outputs": [],
      "source": [
        "def launch_website():\n",
        "  print (\"Click this link to try your web app:\")\n",
        "  if ngrok.get_tunnels():\n",
        "    ngrok.kill()\n",
        "  tunnel = ngrok.connect() # The URL to connect to\n",
        "  print (tunnel.public_url)\n",
        "  !streamlit run --server.port 80 app.py >/dev/null # Connect to the URL through Port 80 (>/dev/null hides outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUwUJ7Mtq8eD"
      },
      "source": [
        "Run the following code now to use our function. Click on the first link, and hit the `Visit Site` button to view your site!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNTZdEYBiWb4"
      },
      "outputs": [],
      "source": [
        "launch_website()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwaVQ-muvOOk"
      },
      "source": [
        "Now the question remains, how do we connect our computer vision models to our website?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyOpoqtwMKVX"
      },
      "source": [
        "### Let's review how we might want to predict on an image:\n",
        "\n",
        "First, we'd like to preprocess our image by resizing it to a constant fixed size of 64x64 for our model to take in. Here are examples of how you might do this:\n",
        "```python\n",
        "image.resize((x, y))\n",
        "```\n",
        "Afterwards, you'd want to transform our image into an array - a numerical format where each number in the array represents the intensity of a pixel.\n",
        "```python\n",
        "img_to_array(image)\n",
        "```\n",
        "And finally, you need to expand the dimensions of the model.\n",
        "```python\n",
        "np.expand_dims(image_array, axis=0)\n",
        "```\n",
        "Most deep learning models are designed to make predictions on batches of data rather than a single sample. When you want to predict the class of a single image, you still need to make it look like a batch, but with size 1. Expanding the dimensions of the array along `axis=0` effectively adds a new dimension at the start of the array, turning the shape of the array from `(height, width, channels)` to `(1, height, width, channels)`.\n",
        "<br>\n",
        "\n",
        "Now, try putting this all together and make predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oe6wYE2OCKo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def predict_image(img):\n",
        "    # Preprocess the image\n",
        "    img = img.resize((64, 64))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "    # Fill in Class Labels (i.e., what are our four classes?)\n",
        "    class_labels = ['Attentive', 'DrinkingCoffee', 'UsingRadio', 'UsingMirror' ] # FILL THE LIST IN\\\n",
        "    return class_labels[predicted_class]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IStMpsNO_Jx"
      },
      "source": [
        "You can now test this out by uploading an image via the left sidebar to Colab's files of a distracted (or attentive) driver and get its image path to make a prediction! Feel free to Google an image, or use this example image to test this out! (right-click the image and click 'Save As...', and upload to the file explorer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEqO-x6YTSWA"
      },
      "source": [
        "![](https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Driver%20Distraction%20Detection/distracted_driver.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y8m8mFrPMTr"
      },
      "outputs": [],
      "source": [
        "image = Image.open('/content/distracteddriving.jpg').convert('RGB')\n",
        "print(predict_image(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXb6UsVqQHzI"
      },
      "source": [
        "## Now, let's put this all together!\n",
        "To create a button with Streamlit to upload our images, we can use\n",
        "```python\n",
        "uploaded_file = st.file_uploader(\"Prompt\", type=[\"jpg\", \"png\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGcORKTmQs2H"
      },
      "source": [
        "Fill out the rest of this code to build a simple webapp to predict off uploaded images!\n",
        "\n",
        "*Tip*: When we add in the `%%writefile` magic command, Colab removes all syntax highlighting, which makes the code a lot harder to read! If you'd like, you can comment out that `%%writefile` line by adding a `#` at the beginning to restore the syntax highlighting as you edit. Make sure you remove that `#` before you run this, so that your `app.py` file is actually overwritten!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGiaUivoK6tM"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('vgg16.h5')\n",
        "\n",
        "def predict_image(img):\n",
        "    img = img.resize((64, 64))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions)\n",
        "\n",
        "    # Fill in Class Labels (i.e., what are our four classes?)\n",
        "    class_labels = ['Attentive', 'DrinkingCoffee', 'UsingRadio', 'UsingMirror' ] # FILL THE LIST IN\\\n",
        "    return class_labels[predicted_class]\n",
        "\n",
        "st.title(\"Distracted Driving is bad\")\n",
        "\n",
        "# Create your upload image button\n",
        "uploaded_file = uploaded_file = st.file_uploader(\"Prompt\", type=[\"jpg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file).convert('RGB')\n",
        "    st.image(image, caption='Uploaded Image.', use_column_width=True)\n",
        "\n",
        "    # Make your prediction here\n",
        "    predictions = predict_image(image)\n",
        "    st.write(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usCdpKHkg0t0"
      },
      "source": [
        "While writing your prediction code, recall that our class labels are as follows:\n",
        "```\n",
        "['Attentive', 'Distracted - Drinking Coffee', 'Distracted - Using Mirror', 'Distracted - Using Radio']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np8E9t5JU6_4"
      },
      "outputs": [],
      "source": [
        "launch_website()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w_hhuuFRd_i"
      },
      "source": [
        "## (Challenge) Adding a Saliency Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQiD_i0GR2W9"
      },
      "source": [
        "As before, we can import our saliency map with a similar function, but modified for Streamlit. To do this, we can create a new file, called `utils.py`. We've included additional functions, such as `saliency_map_to_rgb` and `overlay_saliency_map` to overlay our map on our image!\n",
        "\n",
        "Adjust the alpha value in `overlay_saliency_map` to change the opacity of the saliency map overlay!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQDzFnfGR1J0"
      },
      "outputs": [],
      "source": [
        "#@title Saliency Map\n",
        "%%writefile utils.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  # for colormap\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.utils import normalize\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "\n",
        "def get_saliency_map(model, input_image, class_index):\n",
        "    \"\"\"\n",
        "    generate saliency map for a given class and image\n",
        "    \"\"\"\n",
        "    # define loss function\n",
        "    score = CategoricalScore(class_index)\n",
        "\n",
        "    # create saliency object\n",
        "    saliency = Saliency(model,\n",
        "                        model_modifier=ReplaceToLinear(),\n",
        "                        clone=True)\n",
        "\n",
        "    # generate saliency map\n",
        "    saliency_map = saliency(score, input_image)\n",
        "    saliency_map = normalize(saliency_map)\n",
        "\n",
        "    return saliency_map\n",
        "\n",
        "def saliency_to_rgb(saliency_map):\n",
        "    \"\"\"\n",
        "    convert saliency map to rgb using blue-red color scheme\n",
        "    \"\"\"\n",
        "    # normalize the saliency map for better visualization\n",
        "    saliency_map_normalized = (saliency_map - np.min(saliency_map)) / (np.max(saliency_map) - np.min(saliency_map))\n",
        "\n",
        "    saliency_map_colored = plt.get_cmap('jet')(saliency_map_normalized)[:, :, :3]  # exclude alpha channel\n",
        "    saliency_map_colored = (saliency_map_colored * 255).astype(np.uint8)  # convert to uint8 for displaying as an image\n",
        "\n",
        "    return saliency_map_colored\n",
        "\n",
        "def overlay_saliency_map(image, saliency_map, alpha=0.8):\n",
        "    \"\"\"\n",
        "    overlay saliency map on image\n",
        "    \"\"\"\n",
        "    image = image[0].astype(np.uint8)\n",
        "    saliency_map = saliency_map[0]\n",
        "    saliency_rgb = saliency_to_rgb(saliency_map)\n",
        "    return (image * (1 - alpha) + saliency_rgb * alpha).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qmkwVqcSEJZ"
      },
      "source": [
        "In Python, you can use functions from other files by importing them! For example, if we had a file called `helper.py` where we wanted to use their `hello_world()` function:\n",
        "```python\n",
        "%%writefile helper.py\n",
        "\n",
        "def hello_world():\n",
        "    print(\"hello world!\")\n",
        "```\n",
        "We can add the following to our app.py file to ***import*** and use it!\n",
        "```python\n",
        "%%writefile app.py\n",
        "from helper import hello_world\n",
        "\n",
        "hello_world()\n",
        "```\n",
        "Now, try importing our saliency map function to display saliency maps! <br> **Note that you will have to resize and expand your dimensions before inputting your image into the saliency_map function!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qplbI-ilSsjV"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from utils import *\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('vgg16.h5')\n",
        "\n",
        "def predict_image(img):\n",
        "    img = img.resize((64, 64))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "    # Fill in Class Labels (i.e., what are our four classes?)\n",
        "    class_labels = ['Attentive', 'DrinkingCoffee', 'UsingRadio', 'UsingMirror' ] # FILL THE LIST IN\\\n",
        "    return class_labels[predicted_class]\n",
        "\n",
        "st.title(\"Distracted Driving is bad\")\n",
        "\n",
        "# Create your upload image button\n",
        "uploaded_file = uploaded_file = st.file_uploader(\"Prompt\", type=[\"jpg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file).convert('RGB')\n",
        "    st.image(image, caption='Uploaded Image.', use_column_width=True)\n",
        "\n",
        "    # Make your prediction here\n",
        "    predictions = predict_image(image)\n",
        "    st.write(predictions)\n",
        "\n",
        "    img = image.resize((64, 64))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "    saliency_map = get_saliency_map(model, img_array, predicted_class)\n",
        "    overlay = overlay_saliency_map(img_array, saliency_map, alpha=0.6)\n",
        "    st.image(overlay, caption='Uploaded Image.', use_column_width=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo-6H___U3Of"
      },
      "outputs": [],
      "source": [
        "launch_website()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}